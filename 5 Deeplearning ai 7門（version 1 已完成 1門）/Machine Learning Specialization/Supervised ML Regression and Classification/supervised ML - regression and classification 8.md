- ### cost function for logistic regression
	- #### cost function for logistic regression
	- ![[Pasted image 20260218182418.png]]
		- 二元分類問題（binary classification）
			- 每一列是一筆訓練資料（例如一位病患）
			- 每筆資料包含：
				- 多個特徵 $x_1, x_2, ..., x_n$
					- 例如：腫瘤大小、年齡等
				- 一個標籤 $y$
					- $y∈{0,1}$
		- Logistic regression 模型
			- $$f_{w,b}(x) = g(z) = \frac{1}{1 + e^{-(\mathbf{w}\cdot\mathbf{x}+b)}}$$
				- sigmoid 函數
					- 輸出值介於 0 到 1
					- 可被解釋為「屬於類別 1 的機率」
		- 如何選擇 $w$ 與 $b$？
			- 引出「成本函數（cost function）」的設計問題
	- ![[Pasted image 20260218182555.png]]
		- 以 mean squared error（MSE） 為 成本函數（cost function）的問題
			- 若為 linear regression  $$f_{w,b}(x) = w \cdot x + b$$
				- cost function 若為 MSE 
					- $$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} \frac{1}{2} \left( f\left(x^{(i)}\right) - y^{(i)} \right)^2$$
					- convex（凸函數）
					- gradient descent 一定收斂到 global minimum
			- 若為 logistic regression $$f_{w,b}(x) = g(z) = \frac{1}{1 + e^{-(\mathbf{w}\cdot\mathbf{x}+b)}}$$
				- cost function 若仍為 MSE
					- 因 sigmoid 本身 **非 凸函數**
					- 故 其 $J(w, b)$
						- non-convex（非 凸函數）
						- 會出現 local minima
						- gradient descent 可能卡住
	- ![[Pasted image 20260218182629.png]]
		- Logistic loss（當 y=1）
			- 新 cost function（Logistic loss / cross entropy loss）
				- $$L(f(x), y) = \begin{cases} - \log(f(x)) & \text{if } y = 1 \\ - \log(1 - f(x)) & \text{if } y = 0 \end{cases}$$
				- 模型對正確類別的機率取 log，再取負號
			- 當 y=1， Logistic loss 為 $- \log(f(x))$
				- 當 $f(x) \to 1$
					- loss $\to 0$
					- 表示預測很好
				- 當 $f(x) \to 0$
					- loss $\to \infty$
					- 嚴重懲罰
			- eg
				- 若真實是惡性腫瘤（y=1）：
					- 預測 0.9 → loss 很小
					- 預測 0.5 → loss 中等
					- 預測 0.01 → loss 非常大
	- ![[Pasted image 20260218182710.png]]
		- Logistic loss（當 y=0）
			- 當 y=0， Logistic loss 為 $- \log(1 - f(x))$
				- 當 $f(x) \to 0$
					- loss $\to 0$
					- 表示預測很好
				- 當 $f(x) \to 1$
					- loss $\to \infty$
					- 嚴重懲罰
			- eg
				- 若真實是良性腫瘤（y=0）：
					- 預測 0.01 → loss 很小
					- 預測 0.5 → loss 中等
					- 預測 0.9 → loss 非常大
	- ![[Pasted image 20260218182801.png]]
		- 新 cost function（Logistic loss）
			- $$J(w, b) = \frac{1}{m} \sum_{i=1}^{m} L\left(f\left(x^{(i)}\right), y^{(i)}\right)$$
			- convex
				- 因
					- 分段不是因為 $w, b$ 改變而跳來跳去
					- 而是因為資料標籤不同，選到不同的那條公式
				- 故 
					- 對於某一筆樣本 $i$ 而言，loss 其實是**一個固定的函數**（不是訓練過程中會切來切去）
			- gradient descent 可以找到 global minimum
	- #### simplified cost function for logistic regression
	- ![[Pasted image 20260226232245.png]]
		- $$L\left(f_{\vec{w},b}(x^{(i)}), y^{(i)}\right) = \begin{cases} -\log\left(f_{\vec{w},b}(x^{(i)})\right) & \text{if } y^{(i)} = 1 \\ -\log\left(1 - f_{\vec{w},b}(x^{(i)})\right) & \text{if } y^{(i)} = 0 \end{cases}$$
		- logistic loss 簡化成單一公式 如下
		- $$L = -y^{(i)} \log\left(f(x^{(i)})\right)- \left(1 - y^{(i)}\right)\log\left(1 - f(x^{(i)})\right)$$
		- 若 y = 1，則
		- $$L = -1 \log(f) - 0 = -\log(f)$$
		- 若 y = 0，則
		- $$L = 0 - 1 \log(1 - f) = -\log(1 - f)$$
		- 讓 logistic loss 不用再分兩個 case
		- 實作 gradient descent 會更方便
	- ![[Pasted image 20260226232311.png]]
		- 因 y∈{0,1}
		- 故 
			- y 是一個「開關」
			- (1 − y) 是另一個「互補開關」
		- 這是 machine learning 裡非常典型的 分類 技巧
	- ![[Pasted image 20260226232358.png]]
		- Simplified Cost Function（整體資料的 Cost）
			- 從單筆 logistic loss 推到整體 Cost
			- Step 1：定義 Cost
				- Cost = 所有資料 Loss 的平均
				- $$J(w,b) = \frac{1}{m} \sum_{i=1}^{m} L\left(f(x^{(i)}), y^{(i)}\right)$$
			- Step 2：代入簡化後的 logistic loss
				- 即 logistic regression 標準公式
				- $$J(w,b) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)} \log\left(f(x^{(i)})\right) + \left(1 - y^{(i)}\right) \log\left(1 - f(x^{(i)})\right) \right]$$
		- 為什麼選這個 Cost？
			- 來自 Maximum Likelihood Estimation（MLE）
				- logistic loss（cost function）形成之邏輯順序
					- 我們假設資料服從 Bernoulli 分布
						- 成功機率為 p
						- 失敗機率為 1 - p
						- 常用於建模只有兩種結果的事件，如拋硬幣、產品合格與否
					- 用 Maximum Likelihood 找最好的參數
						- 整體 likelihood：$$\prod_{i=1}^{m} p^{y^{(i)}} (1 - p)^{1 - y^{(i)}}$$
						- 取 log（即  log likelihood）：$$\sum \left[ y \log p + (1 - y)\log(1 - p) \right]$$
						- 最小化 negative log likelihood：$$-\sum \left[ y \log p + (1 - y)\log(1 - p) \right]$$
							- 等價於 最大化 likelihood
							- 即 logistic loss（cost function）
			- 它是 Convex 的
				- 只有一個 global minimum
				- 不會卡在 local minimum
				- Gradient descent 保證收斂
		- 為什麼不用平方誤差？
			- non-convex
			- 產生局部極小值
- ### gradient descent for logistic regression
	- #### gradient descent implementation
	- ![[Pasted image 20260227134640.png]]
		- Logistic Regression 要學什麼？
			- 透過訓練資料，找出最好的權重向量 $\mathbf{w}$ 與偏差 $b$。
		- 給定新輸入 $\vec{x}$，模型輸出 sigmoid function（邏輯函數）的值（即 在參數為 $w, b$ 的情況下，給定 $x$ 時 $y=1$ 的機率）：$$f_{\vec{w},b}(\vec{x})=\frac{1}{1 + e^{-(\vec{w}\cdot \vec{x} + b)}}=P(y=1 \mid \vec{x}; \vec{w}, b)$$
		- Linear Regression vs Logistic Regression
			- Linear Regression
				- 預測連續數值（無機率意義）
				- $f(x) = w \cdot x + b$
			- Logistic Regression
				- 預測機率（有機率意義，即 機率分類模型（probabilistic classifier））
				- $f(x) = \sigma(w \cdot x + b)=\frac{1}{1 + e^{-(\vec{w}\cdot \vec{x} + b)}}=P(y=1 \mid \vec{x}; \vec{w}, b)$
	- ![[Pasted image 20260227135035.png]]
		- 如何找出最好的權重向量 $\mathbf{w}$ 與偏差 $b$？
			- Gradient Descent
		- Cost Function
			- Log Loss（Logistic Loss）
				- 即 Negative Log Likelihood、Binary Cross Entropy
				- $$J(\vec{w}, b)=-\frac{1}{m}\sum_{i=1}^{m}\left[y^{(i)} \log f_{\vec{w},b}\!\left(x^{(i)}\right)+\big(1 - y^{(i)}\big)\log\!\left(1 - f_{\vec{w},b}\!\left(x^{(i)}\right)\right)\right]$$
		- 為什麼不用平方誤差？
			- 若使用 MSE，cost function 會變成 **non-convex**
			- Gradient descent 可能卡在 local minimum
			- Cross entropy 保證 cost function 是 **convex** 使 logistic regression 能穩定收斂
		- 該 Cost Function 之 梯度公式 
			- $$\frac{\partial J}{\partial w_j}=\frac{1}{m}\sum_{i=1}^{m}\left( f\big(x^{(i)}\big) - y^{(i)} \right)x_j^{(i)}$$
			- $$\frac{\partial J}{\partial b}=\frac{1}{m}\sum_{i=1}^{m}\left( f\big(x^{(i)}\big) - y^{(i)} \right)$$
		- 雖然 cost function 不同（平方誤差 vs 交叉熵），logistic regression 梯度公式 居然和Linear Regression 梯度公式 一模一樣！
			- logistic regression 梯度公式 和 Linear Regression 梯度公式 一樣 因素
				- sigmoid 導數會和 log loss 導數結合簡化掉
				- Step 1：定義模型
					- $z = w \cdot x + b$
					- $f(x) = \sigma(z) = \frac{1}{1 + e^{-z}}$
				- Step 2：Cost Function
					- $L = - \left[ y \log(f) + (1-y)\log(1-f) \right]$
				- Step 3：求導（重點）
					- 我們要求：$$\frac{\partial L}{\partial w_j}$$
					- chain rule（鏈式法則）：$$\frac{\partial L}{\partial w_j}=\frac{\partial L}{\partial f}\cdot\frac{\partial f}{\partial z}\cdot\frac{\partial z}{\partial w_j}$$
					- 計算 $\frac{\partial L}{\partial f}$
						- $$\frac{\partial L}{\partial f}=-\left(\frac{y}{f}-\frac{1-y}{1-f}\right)$$
					- 計算 $\frac{\partial f}{\partial z}$
						- $$\frac{d}{dz}\sigma(z)=\sigma(z)\big(1-\sigma(z)\big)$$
						- $$\frac{\partial f}{\partial z}=f(1-f)$$
					- 計算 $\frac{\partial z}{\partial w_j}$
						- $z = w \cdot x + b$
						- $$\frac{\partial z}{\partial w_j}=x_j$$
					- 故$$\frac{\partial L}{\partial w_j}=(f - y)\, x_j$$
		- logistic regression 梯度公式 和 Linear Regression 梯度公式 一樣 but
			- Linear regression 的 f = wx + b
			- Logistic regression 的 f = sigmoid(wx + b)
			- 差別藏在 f 裡面，不在梯度公式
	- ![[Pasted image 20260227135126.png]]
- ### the problem of overfitting
	- #### the problem of overfitting